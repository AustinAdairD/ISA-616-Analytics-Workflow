---
title: " Ames Iowa Housing Analysis"
author: "Austin Adair"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    code_folding: show
    df_print: paged
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache=TRUE,
                      out.width="100%",
                      warning=FALSE,
                      message=FALSE,
                      clean_cache=TRUE)
```

### Ames Iowa Housing Analysis

In this paper I attempted to discern what factors are associated with the highest sales price of a home in Ames Iowa. The data analyzed contains housing sales prices from 2006 through 2010. Access to the database can be gained from the Ames site (http://www.cityofames.org/assessor/) by clicking on “property search” or by accessing the Beacon (http://beacon.schneidercorp.com/Default.aspx) website and inputting Iowa and Ames in the appropriate fields.

The primary purpose for this paper is to provide guidance for - Friedrich Land Development; who has been engaged in real-estate development in Ames Iowa for over 50 years. Specifically, this analysis will give insights into what housing features are associated with the highest correlation to final sales price of a house. This will allow them to better evaluate the true value of houses currently on the market and understand what features should be added to generate the highest ROI. 

The reader can show any code chunk by clicking on the code button. We chose to make the default for the code hidden since we: (a) wanted to improve the readability of this document; and (b) assumed that the readers will not be interested in reading every code chunk. 

### Loading data

```{r}
rm(list = ls()) # Clears global environment
library(pacman)
p_load(DataExplorer, ggplot2, ggcorrplot, dplyr, corrplot, jtools, ggstance, broom.mixed)
data = read.csv("AmesHousing.csv", stringsAsFactors = TRUE) # reads in data set

# Correctly Code Variables
data$Bsmt.Full.Bath=factor(data$Bsmt.Full.Bath)
data$Bsmt.Half.Bath =factor(data$Bsmt.Half.Bath)
data$Full.Bath =factor(data$Full.Bath)
data$Half.Bath =factor(data$Half.Bath)
data$Bedroom.AbvGr = factor(data$Bedroom.AbvGr)
data$Kitchen.AbvGr = factor(data$Kitchen.AbvGr)


```

### Data Overview

In viewing the Ames Housing data in R we can see that there are 2,920 observations of 82 unique variables. The Ames Housing data contains 0 missing columns and 13,960 total missing values (about 5.8% of the total values in the data set are missing). The majority of the missing values came from the following variables; Fireplace.Qu (49% missing), Fence (80% missing), Alley (93% missing), Misc.Fence (96% missing) and Pool.QC (99.6% missing).The the columns with large amounts of missing values will mot contribute any value in building our model and will be removed. 

Next I will find which of our remaining variables are highly correlated to one another. In order to compare correlations in the data a function was generated using logic from Katherine Williams *How to Create a Correlation Matrix with Too Many Variables in R, 2020*. This function returns a simplified correlation plot that  the correlations with a high enough significance level will have a colored circle. Highly correlated variables will be removed from the model once it is built due to the fact that they contain redundant information. 

```{r}
# data Overview
dim(data) # gives the number of variables and observations in the data set
head(data) # gives a glimpse of the head of the data

# plot missing data
plot_missing(data, group = list(Good = 0.05, OK = 0.3, Bad = 0.5, Remove = 1), missing_only = TRUE, geom_label_args = list(), title = NULL, ggtheme = theme_gray(), theme_config = list(legend.position = c("bottom"))) 

# Correlation Plot

corr_simple <- function(data= data,sig=0.5){
  #convert data to numeric in order to run correlations
  #convert to factor first to keep the integrity of the data - each value will become a number rather than turn into NA
df_cor <- data %>% mutate_if(is.character, as.factor)
df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
corr <- cor(df_cor)
  #prepare to drop duplicates and correlations of 1     
corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
corr[corr == 1] <- NA 
  #turn into a 3-column table
corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
corr <- na.omit(corr) 
  #select significant values  
corr <- subset(corr, abs(Freq) > sig) 
  #sort by highest correlation
corr <- corr[order(-abs(corr$Freq)),] 
  #print table
print(corr)
  #turn corr back into matrix in order to plot with 
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
corr_simple(data) # creates a correlation plot including only highly correlated variables 
```

### Details on Data Preprocessing

Now that we understand our data, I will clean it by removing the variables that we identified that were missing least 45% o their values.Then we will impute the impute the mean of numeric values and the mode of categorical values that are missing least 5% of their values. In doing so we are able to retain observations that we would have otherwise omitted. This will give us more information to build our model and will potentially improve its explanatory capabilities. 

Once we have addressed the majority of the missing values, we can then omit any variable that are still missing. In doing so we will only be eliminating 104 observation, which should have a minimal impact on developing a model.  

```{r}
#remove any variables that had least 45% missing data

data<-select(data, -Fireplace.Qu)
data<-select(data, -Fence)
data<-select(data, -Alley)
data<-select(data, -Misc.Feature)
data<-select(data, -Pool.QC)

#impute any variables that had least 5% missing data and was not removed

# Creating Mode function

getmode <- function(data) {
  uniqv <- unique(data)
  uniqv[which.max(tabulate(match(data, uniqv)))]
}

#Garage.Finish
data$M_Garage.Finish<-as.factor(ifelse(is.na(data$Garage.Finish), 1, 0))
data$Garage.Finish[is.na(data$Garage.Finish)]<-getmode(data$Garage.Finish)
      #Garage.Type
data$M_Garage.Type<-as.factor(ifelse(is.na(data$Garage.Type), 1, 0))
data$Garage.Type[is.na(data$Garage.Type)]<-getmode(data$Garage.Type)
      #Garage.Cod
data$M_Garage.Cond<-as.factor(ifelse(is.na(data$Garage.Cond), 1, 0))
data$Garage.Cond[is.na(data$Garage.Cond)]<-getmode(data$Garage.Cond)
      #Garage.Qual
data$M_Garage.Qual<-as.factor(ifelse(is.na(data$Garage.Qual), 1, 0))
data$Garage.Qual[is.na(data$Garage.Qual)]<-getmode(data$Garage.Qual)
      #Garage.Yr.Blt
data$M_Garage.Yr.Blt<-as.factor(ifelse(is.na(data$Garage.Yr.Blt), 1, 0))
data$Garage.Yr.Blt[is.na(data$Garage.Yr.Blt)]<-median(data$Garage.Yr.Blt, na.rm = TRUE)
      #Lot.Frontage
data$M_Lot.Frontage<-as.factor(ifelse(is.na(data$Lot.Frontage), 1, 0))
data$Lot.Frontage[is.na(data$Lot.Frontage)]<-median(data$Lot.Frontage, na.rm = TRUE)
    #SalePrice
data$M_SalePrice<-as.numeric(ifelse(is.na(data$SalePrice), 1, 0))
data$SalePrice[is.na(data$SalePrice)]<-median(data$SalePrice, na.rm = TRUE)

# remove the newly created factors from the data
data<-select(data, -M_SalePrice)
data<-select(data, -M_Lot.Frontage)
data<-select(data, -M_Garage.Yr.Blt)
data<-select(data, -M_Garage.Cond)
data<-select(data, -M_Garage.Type) 
data<-select(data, -M_Garage.Finish)
data<-select(data, -M_Garage.Qual)

# remove missing data so we can run a stepwise regression
data=na.omit(data)
```

### Building A Linear Regression

To build an effective model we are going to create a stepwise regression. The stepwise regression will add and remove variables until an optimal model is created. The stepwise regression determined that there are a total o 173 variables that are statistically signiicant in predicting the sale price of a home in Ames Iowa. 

```{r}
#Creating the first stepwise regression on training data 

full<-lm(SalePrice~., data = data) # creating full linear regression

null<-lm(SalePrice~1, data=data) # Creating null linear regression

lm.step1<-step(null, scope=list(lower=null, upper=full), direction="both", trace=0) 

```

### Removing Correlate Variables 

Now that we have found what variables are statistically significant, we need to remove highly correlated variables from the model. Highly correlated variables are redundant in the seance that they communicate repetitive information. We will also remove the variable neighborhoods as it contains 28 distinct levels which complicates the model and adds unnecessary degrees of freedom. Likewise, removing the correlated variables and neighborhoods in our model has a minimal impact in the adjusted R squared values while creating a simpler model. 

Our simpler model contains a ar ewer 103 variables while maintaining a relatively high R squared value. The new model has an adjusted R squared value of 0.84 in comparison to the previous model which had an adjusted R square value of 0.91. 

This means that in our simpler model 84% of the variance in the final sales price of a house in Ames Iowa is explained by our model.

```{r}
lm = lm(SalePrice ~ Overall.Qual + BsmtFin.SF.1 + Roof.Matl + MS.SubClass + Bsmt.Exposure + Condition.2 + Sale.Condition + Garage.Area + Overall.Cond + Bsmt.Qual + Total.Bsmt.SF + Lot.Area + Pool.Area + Screen.Porch + Functional + Exterior.1st + Land.Contour + Bsmt.Full.Bath + Mas.Vnr.Area + Condition.1 + Fireplaces + Garage.Yr.Blt + Mas.Vnr.Type + BsmtFin.SF.2 + Wood.Deck.SF + Garage.Finish + BsmtFin.Type.1 + MS.Zoning + Land.Slope + Garage.Cars + Kitchen.AbvGr + Low.Qual.Fin.SF + Utilities, data = data)

```

### Create A Graph Showing The Coefficient Values

This barplot depicts the top 6 coefficient's that have the greatest impact on the final sale price on a home in Iowa. It is important to now that all of these coefficients are categories for the roof material of a house. This will be a crucial piece of information for Friedrich Land Development to keep in mind when they build/renovate houses. 

```{r}
lm2 = abs(lm$coefficients)
sort(lm2, decreasing = TRUE)
lm2 = (head(sort(lm2, decreasing = TRUE), 6))
barplot(lm2,
        main = "Top 6 Coefficients (ABS)",
        names.arg = c("Shngl", "Roll", "Shake", "CompShg", "Membran", "Metal"))
```

### Create A Graph Sjowing The Prediction Ability Of The Model 

The histogram of the residuals of a model is a useful proxy for the predictive power of a model. Overall, our model's residuals are normally distributed and centered around 0. This, alongside our large R squared value, means that our linear regression model is an appropriate predictor for the final sales price of a home in Ames Iowa.

```{r}
ggplot(data=data, aes(lm$residuals)) +
  geom_histogram(binwidth = 1, color = "black", fill = "purple4") +
  theme(panel.background = element_rect(fill = "white"),
        axis.line.x=element_line(),
        axis.line.y=element_line()) +
  ggtitle("Histogram for Model Residuals")
```

### Conclusion

In conclusion, our model was successful in answering the original question of what factors are associated with the highest sales price of a home in Ames Iowa. Although our final model contained a plethora (103) of statistically significant variables there were some that had exceptionally high correlations. 

In analyzing our model we can conclude that the most important factor in predicting the final sale price of a house would be the roofing materiel used. In a business sense, we will need to do additional exploratory research to understand why roofing material is such an important actor in predicting tyhe inal sale price. This information can help guide Friedrich Land Development to determine what roofing material they can utilize for the best ROI.  

Furthermore, our linear regression is able to give insights into several other features of a house and is able to quantify the value (in dollars) of said feature. This can be a good tool in both evaluating the current price of a home on the market, but also in determining what renovations to add next. 

Moving forward, there are some additional questions that are still unanswered:

 * Is this model useful at explaining the predicted sales price of homes outside of Iowa?

 * Is this model still relevant in 2020? 

 * Have consumers preference in housing features shifted significantly from when the data was collected (2006-2010) until today? 


